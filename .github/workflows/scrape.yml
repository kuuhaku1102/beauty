name: Scrape to MySQL (via SSH)

on:
  workflow_dispatch:
    inputs:
      target_urls:
        description: "Comma or newline separated URLs (optional). If empty, use start_id/end_id."
        required: false
        default: |
      start_id:
        description: "Start clinic ID (e.g. 1). Ignored if target_urls is set."
        required: false
        default: "1"
      end_id:
        description: "End clinic ID (e.g. 9999). Ignored if target_urls is set."
        required: false
        default: "9999"

# 複数Runが衝突しないように
concurrency:
  group: scrape-${{ github.ref }}
  cancel-in-progress: false

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 360

    env:
      # 環境変数（inputs）
      START_ID: ${{ inputs.start_id }}
      END_ID: ${{ inputs.end_id }}
      TARGET_URLS: ${{ inputs.target_urls }}
      OUTPUT_DIR: "output"

      # MySQL接続情報（Secrets）
      DB_USER: ${{ secrets.DB_USER }}
      DB_PASS: ${{ secrets.DB_PASS }}
      DB_NAME: ${{ secrets.DB_NAME }}

      # SSH接続情報（Secrets）
      SSH_HOST: ${{ secrets.SSH_HOST }}
      SSH_PORT: ${{ secrets.SSH_PORT }}
      SSH_USER: ${{ secrets.SSH_USER }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Prepare output folder
        run: |
          mkdir -p output
          echo "run_id=${{ github.run_id }}"   >  output/_meta.txt
          echo "run_number=${{ github.run_number }}" >> output/_meta.txt
          echo "started_at=$(date -u +%FT%TZ)" >> output/_meta.txt

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 pandas sqlalchemy pymysql

      # ===== SSHトンネル作成 =====
      - name: Setup SSH key
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          ssh-keyscan -p ${{ secrets.SSH_PORT }} -H ${{ secrets.SSH_HOST }} >> ~/.ssh/known_hosts

      - name: Create SSH tunnel
        run: |
          echo "Opening SSH tunnel to ${{ secrets.SSH_HOST }}:${{ secrets.SSH_PORT }} ..."
          nohup ssh -i ~/.ssh/id_rsa \
            -p ${{ secrets.SSH_PORT }} \
            -o ServerAliveInterval=60 \
            -o ExitOnForwardFailure=yes \
            -N -L 3307:127.0.0.1:3306 \
            ${{ secrets.SSH_USER }}@${{ secrets.SSH_HOST }} > tunnel.log 2>&1 &
          sleep 5
          echo "✅ SSH tunnel established (port 3307)"

      # ===== URL計画確認 =====
      - name: Show targets and save plan
        run: |
          set -e
          mkdir -p output
          if [ -n "${TARGET_URLS// }" ]; then
            echo "Using TARGET_URLS input"
            echo "${TARGET_URLS}" | tr ',\r' '\n' | sed 's/^[ \t]*//;s/[ \t]*$//' | grep -E '^https?://' | sort -u > output/targets.txt
          else
            echo "Using START_ID..END_ID"
            start=${START_ID:-1}
            end=${END_ID:-9999}
            : > output/targets.txt
            for ((i=start; i<=end; i++)); do
              printf "https://kireireport.com/clinics/%04d\n" "$i" >> output/targets.txt
            done
          fi
          echo "Planned targets:"
          head -n 20 output/targets.txt || true
          wc -l output/targets.txt || true

      # ===== メインスクレイパー実行 =====
      - name: Run scraper (MySQL mode)
        working-directory: beauty
        run: python scripts/scrape.py

      # ===== 成果物アップロード =====
      - name: Upload artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scrape-output
          path: |
            output/*.csv
            output/*.json
            output/*.txt
          if-no-files-found: warn

      # ===== SSHログ確認 =====
      - name: Show SSH logs
        if: always()
        run: cat tunnel.log || true
